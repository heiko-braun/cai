{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8c4600-d3c8-4bda-aa90-ce9dfd6b634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import (    \n",
    "    Any    \n",
    ")\n",
    "\n",
    "\n",
    "from conf.constants import QDRANT_KEY, QDRANT_URL\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.vectorstores.utils import maximal_marginal_relevance\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "\n",
    "# create an embedding using openai\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   resp = OpenAI().embeddings.create(input = [text], model=model)\n",
    "   return resp.data[0].embedding\n",
    "\n",
    "def document_from_scored_point(        \n",
    "        scored_point: Any,\n",
    "        content_payload_key: str,\n",
    "        metadata_payload_key: str,\n",
    "    ) -> Document:\n",
    "        return Document(\n",
    "            page_content=scored_point.payload.get(content_payload_key),\n",
    "            metadata=scored_point.payload.get(metadata_payload_key) or {},\n",
    "        )\n",
    "    \n",
    "# query the vector store\n",
    "def query_qdrant(query, collection_name, fetch_k=20, top_k=5):\n",
    "    \n",
    "    embedding = get_embedding(text=query)\n",
    "\n",
    "    qdrant_client = QdrantClient(\n",
    "        QDRANT_URL,\n",
    "        api_key=QDRANT_KEY,\n",
    "    )\n",
    "\n",
    "    results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=(embedding),\n",
    "        with_payload=True,\n",
    "        with_vectors=True,\n",
    "        limit=fetch_k,\n",
    "    )\n",
    "\n",
    "    ## The MMR impl used with retriever(search_type='mmr')    \n",
    "    embeddings = [result.vector for result in results]\n",
    "    \n",
    "    mmr_selected = maximal_marginal_relevance(\n",
    "            np.array(embedding), embeddings, k=top_k, lambda_mult=0.75\n",
    "        )\n",
    "    \n",
    "    return [\n",
    "        (\n",
    "            document_from_scored_point(\n",
    "                scored_point=results[i], \n",
    "                content_payload_key=\"page_content\", \n",
    "                metadata_payload_key=\"metadata\"\n",
    "            ),\n",
    "            results[i].score,\n",
    "        )\n",
    "        for i in mmr_selected\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff186b7-a7fe-46bf-806b-578b9549c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "COLLECTION = \"spring_reference_2\"\n",
    "QUERY = \"streaming upload mode\"\n",
    "NUM_RESULTS = 5\n",
    "query_results = query_qdrant(        \n",
    "    query=QUERY, \n",
    "    collection_name=COLLECTION,\n",
    "    top_k=NUM_RESULTS\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(columns=['score', 'page_ref', 'entities', 'content', 'content_size'])\n",
    "\n",
    "for i, article in enumerate(query_results):    \n",
    "    doc = article[0]\n",
    "    score = article[1]\n",
    "    \n",
    "    data = {        \n",
    "        \"score\": round(score, 3),\n",
    "        \"page_ref\": doc.metadata[\"page_number\"],\n",
    "        \"entities\": doc.metadata[\"entities\"],\n",
    "        \"content\": doc.page_content,\n",
    "        \"content_size\": len(doc.page_content)              \n",
    "    }\n",
    "    df_new_rows = pd.DataFrame(data, index=[i])\n",
    "    df = pd.concat([df, df_new_rows])   \n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31ebce-d03c-4f18-b021-2eb353afcf70",
   "metadata": {},
   "source": [
    "# General structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30f9296-19d0-47c4-af4d-8004c7af9d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>page_ref</th>\n",
       "      <th>entities</th>\n",
       "      <th>content</th>\n",
       "      <th>content_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788</td>\n",
       "      <td>/page_206.txt</td>\n",
       "      <td>The top 20 entities mentioned in the given con...</td>\n",
       "      <td>\"The default size for a batch is 1 Mb, but you...</td>\n",
       "      <td>2038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.757</td>\n",
       "      <td>/page_497.txt_0</td>\n",
       "      <td>The top 20 entities mentioned in the given con...</td>\n",
       "      <td>\"Raw Mode:\\n Attachments are not supported as ...</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.780</td>\n",
       "      <td>/page_196.txt</td>\n",
       "      <td>Here are the top 20 entities mentioned in the ...</td>\n",
       "      <td>\"restartingPolicy\\n(producer)\\nThe restarting ...</td>\n",
       "      <td>1604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.761</td>\n",
       "      <td>/page_660.txt</td>\n",
       "      <td>Based on the context provided, the top 20 enti...</td>\n",
       "      <td>\"Name\\nDescription\\nDefaul\\nt\\nType\\nbinary\\n ...</td>\n",
       "      <td>1717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.752</td>\n",
       "      <td>/page_711.txt</td>\n",
       "      <td>The top 20 entities mentioned in the given con...</td>\n",
       "      <td>\"producerStrategy\\n(producer)\\nThe mode used t...</td>\n",
       "      <td>1557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score         page_ref                                           entities  \\\n",
       "0  0.788    /page_206.txt  The top 20 entities mentioned in the given con...   \n",
       "1  0.757  /page_497.txt_0  The top 20 entities mentioned in the given con...   \n",
       "2  0.780    /page_196.txt  Here are the top 20 entities mentioned in the ...   \n",
       "3  0.761    /page_660.txt  Based on the context provided, the top 20 enti...   \n",
       "4  0.752    /page_711.txt  The top 20 entities mentioned in the given con...   \n",
       "\n",
       "                                             content content_size  \n",
       "0  \"The default size for a batch is 1 Mb, but you...         2038  \n",
       "1  \"Raw Mode:\\n Attachments are not supported as ...         2483  \n",
       "2  \"restartingPolicy\\n(producer)\\nThe restarting ...         1604  \n",
       "3  \"Name\\nDescription\\nDefaul\\nt\\nType\\nbinary\\n ...         1717  \n",
       "4  \"producerStrategy\\n(producer)\\nThe mode used t...         1557  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.iloc[:NUM_RESULTS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bcb6bf-87dc-4db3-b671-c8f1247c7849",
   "metadata": {},
   "source": [
    "# Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfe2139-e1a1-4903-adf4-55d186e5e750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 0 : \n",
      " The top 20 entities mentioned in the given context are:\n",
      "1. S3 bucket\n",
      "2. producer route\n",
      "3. upload\n",
      "4. progressive naming strategy\n",
      "5. keyname\n",
      "6. batch\n",
      "7. restartingPolicy\n",
      "8. files\n",
      "9. messages\n",
      "10. streaming upload\n",
      "11. endpoint\n",
      "12. exchange\n",
      "13. NPE (NullPointerException)\n",
      "14. AWS2S3EndpointBuilder\n",
      "15. batchMessageNumber\n",
      "16. lastPart\n",
      "17. 1 Mb\n",
      "18. 20 messages\n",
      "19. 70 messages\n",
      "20. 25 messages\n",
      "\n",
      "Summary:\n",
      "The context discusses the configuration and usage of the AWS S3 storage service with a focus on streaming upload, batch sizes, and naming strategies. It provides a detailed example of starting, stopping, restarting, and overriding the uploading process while creating and managing files within the S3 bucket. Additionally, it emphasizes the importance of specific configurations, such as keyName and restartingPolicy, in the context of the streaming upload mode. \n",
      "\n",
      "\n",
      "Item 1 : \n",
      " The top 20 entities mentioned in the given context are:\n",
      "\n",
      "1. CXF_RAW Mode\n",
      "2. MTOM\n",
      "3. Camel Message APIs\n",
      "4. SOAPMessage\n",
      "5. String converter\n",
      "6. STREAMING SUPPORT IN PAYLOAD MODE\n",
      "7. PAYLOAD mode\n",
      "8. DOM parsed\n",
      "9. javax.xml.transform.Source\n",
      "10. Simple proxy\n",
      "11. WS-Security\n",
      "12. Endpoint property\n",
      "13. Component property\n",
      "14. Global system property\n",
      "15. Camel-CXF component\n",
      "16. CXF dispatch mode\n",
      "17. SoapHeader\n",
      "18. ArrayList\n",
      "19. Elements\n",
      "20. exchange.getOut()\n",
      "\n",
      "Summary:\n",
      "The context discusses various features and modes of the Apache Camel CXF component, focusing on the support for MTOM, streaming of incoming messages in payload mode, and the usage of the generic CXF dispatch mode. It also covers the ways to control streaming, handling invalid incoming XML, and the options to manipulate properties for streaming. \n",
      "\n",
      "\n",
      "Item 2 : \n",
      " Here are the top 20 entities mentioned in the given context:\n",
      "1. restartingPolicy\n",
      "2. storageClass\n",
      "3. streamingUploadMode\n",
      "4. awsKMSKeyId\n",
      "5. useAwsKMS\n",
      "6. useCustomerKey\n",
      "7. backoffErrorThreshold\n",
      "8. backoffIdleThreshold\n",
      "9. backoffMultiplier\n",
      "10. delay\n",
      "11. streaming\n",
      "12. upload\n",
      "13. bucket\n",
      "14. timeout\n",
      "15. KMS\n",
      "16. enabled\n",
      "17. Customer Key\n",
      "18. backoffMultipler\n",
      "19. polls\n",
      "20. milliseconds\n",
      "\n",
      "Summary of the context:\n",
      "The context provided is related to configuration parameters for streaming upload mode and the scheduling of polling consumers in the AWS S3 storage service. It includes details about different parameters such as restarting policy, storage class, streaming upload mode, AWS KMS key ID, backoff thresholds, delay, and other advanced settings. The information is primarily focused on the configuration options and enums used in the context of AWS S3 storage service and related functionalities. \n",
      "\n",
      "\n",
      "Item 3 : \n",
      " Based on the context provided, the top 20 entities mentioned are:\n",
      "\n",
      "1. file transfer mode\n",
      "2. BINARY\n",
      "3. ASCII\n",
      "4. encoding\n",
      "5. consumer\n",
      "6. encodings\n",
      "7. Camel\n",
      "8. charset\n",
      "9. file content\n",
      "10. remote FTP server\n",
      "11. disconnect\n",
      "12. doneFileName\n",
      "13. Producer\n",
      "14. done file\n",
      "15. original file\n",
      "16. folder\n",
      "17. file name\n",
      "18. dynamic placeholders\n",
      "19. Chapter 34\n",
      "20. 657\n",
      "\n",
      "Summary:\n",
      "The context provided is a description of various configuration options related to file transfer in the context of Apache Camel. It explains different parameters and their usage in controlling the file transfer mode, encoding, disconnecting from the FTP server, and handling special files such as a done file. It also mentions specific considerations for producers and consumers, along with dynamic placeholder usage. The context is a technical explanation of FTP configuration options within the Apache Camel framework. \n",
      "\n",
      "\n",
      "Item 4 : \n",
      " The top 20 entities mentioned in the given context are:\n",
      "\n",
      "1. gRPC\n",
      "2. SIMPLE mode\n",
      "3. STREAMING mode\n",
      "4. streamRepliesTo\n",
      "5. String\n",
      "6. userAgent\n",
      "7. lazyStartProducer\n",
      "8. CamelContext\n",
      "9. routes\n",
      "10. boolean\n",
      "11. synchronous\n",
      "12. security\n",
      "13. authenticationType\n",
      "14. SSL/TLS negotiation\n",
      "15. NONE\n",
      "16. GOOGLE\n",
      "17. JWT\n",
      "18. GrpcProducerStrategy\n",
      "19. streamRepliesTo\n",
      "20. GrpcAuthType\n",
      "\n",
      "Summary:\n",
      "The context provided describes various parameters and configurations related to the producer strategy, communication mode, user agent, lazy start, synchronous processing, and security settings for a gRPC server. It also mentions authentication methods, SSL/TLS negotiation, and enum values for specific types. The focus is on defining the behavior and settings for communicating with a remote gRPC server within the given software environment. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entities = df['entities'].tolist()\n",
    "for i, item in enumerate(entities):        \n",
    "    print(\"Item\", str(i),\": \\n\", item, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e5e27a-d6cc-4cea-9238-46d5ea1fa71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The default size for a batch is 1 Mb, but you can adjust it according to your requirements.\n",
      "When you’ll stop your producer route, the producer will take care of flushing the remaining buffered\n",
      "messaged and complete the upload.\n",
      "In Streaming upload you’ll be able restart the producer from the point where it left. It’s important to note\n",
      "that this feature is critical only when using the progressive naming strategy.\n",
      "By setting the restartingPolicy to lastPart, you will restart uploading files and contents from the last part\n",
      "number the producer left.\n",
      "Example\n",
      "1\n",
      ". \n",
      "Start the route with progressive naming strategy and keyname equals to camel.txt, with\n",
      "batchMessageNumber equals to 20, and restartingPolicy equals to lastPart - Send 70\n",
      "messages.\n",
      "2\n",
      ". \n",
      "Stop the route\n",
      "3\n",
      ". \n",
      "On your S3 bucket you should now see 4 files: * camel.txt\n",
      "camel-1.txt\n",
      "camel-2.txt\n",
      "camel-3.txt\n",
      "The first three will have 20 messages, while the last one only 10.\n",
      "4\n",
      ". \n",
      "Restart the route.\n",
      "5\n",
      ". \n",
      "Send 25 messages.\n",
      "6\n",
      ". \n",
      "Stop the route.\n",
      "7\n",
      ". \n",
      "You’ll now have 2 other files in your bucket: camel-5.txt and camel-6.txt, the first with 20\n",
      "messages and second with 5 messages.\n",
      "8\n",
      ". \n",
      "Go ahead\n",
      "This won’t be needed when using the random naming strategy.\n",
      "On the opposite you can specify the override restartingPolicy. In that case you’ll be able to override\n",
      "whatever you written before (for that particular keyName) on your bucket.\n",
      "NOTE\n",
      "In Streaming upload mode the only keyName option that will be taken into account is the\n",
      "endpoint option. Using the header will throw an NPE and this is done by design. Setting\n",
      "the header means potentially change the file name on each exchange and this is against\n",
      "the aim of the streaming upload producer. The keyName needs to be fixed and static.\n",
      "The selected naming strategy will do the rest of the of the work.\n",
      "bucket\"\n",
      ").streamingUploadMode(\n",
      "true\n",
      ").batchMessageNumber(\n",
      "25\n",
      ").namingStrategy(AWS2S3EndpointBu\n",
      "ilderFactory.AWSS3NamingStrategyEnum.progressive).keyName(\n",
      "\"\n",
      "{{kafkaTopic2}}/{{kafkaTopic2}}.txt\"\n",
      "));\n",
      "CHAPTER 9. AWS S3 STORAGE SERVICE\n",
      "203\"\n"
     ]
    }
   ],
   "source": [
    "print(df['content'].tolist()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
